# LLM空間制御 復習単語ノート

---

## 🔥 このノートのルール

- ユーザーが書いた単語の詳細内容が明らかに間違っていたらAIは直す。
- ただし、ユーザーが指定した単語のみ編集する（他の単語は勝手に触らない）。
- ユーザーはイメージ優先で書く。AIは正確さで上書きしない（これはユーザーの脳内地図）。
- ただし明らかな誤りは指摘のうえ修正を検討する。
- とにかく面白く書く。笑えるけど伝わるが正義。
- 厳密さより「わかる！」を優先。レベル感は“安い・早い・うまい”。

---

# LLM空間制御 復習単語ノート

## 基本構造

- Embedding

  空間に浮かんでいる単語の駅、次元数に応じた座標がくっついてる、

  JavaでいうMap\<String,List\<float>>

  めっちゃ奥が深い

- Linear

  Embaddingが駅ならこっちは線路、イメージ的には次の単語を見る方向、物理的にはEmbeddingと同じサイズ（でも向きは逆）の配列みたいになってて座標がくっついてる

  ベクトルをスコアに変換する「どう読むか」を決める層ってAIくんは言うけど要はEmbeddingの次の方向が書いてある。

  こいつもヤバイ奥が深い

- Logits

  EmbaddingとLinearをなんかうまいこと計算したスコア（結果）の生データ、要は「Linearの出力そのもの（ってAIくんが補記しろって言うから補記）」こいつをSoftmaxにぶん投げるとめっちゃいい感じに単語を選べる仕組みを作ってくれる

- Softmax

  Linear→Logitsって出てきたデータをこいつに投げると複数ある単語が候補として上がってくる、その結果を競争させられて全体を足すと1になるように競争ランキング結果を作ってくれる

  なんか黒幕みたいなやつ。実は性格悪いかも。

- CrossEntropyLoss

  正解の確率を上げる損失関数って書いてあったけど、要はheloの例で今hで次eのときにeを勝たせて他のを負けにしたいってときに使う関数。

- Loss

  モデルがどれだけ「今の答えはズレてるか」を数値で怒られている状態。

  小さい＝お、だいぶ合ってきたやん？
  大きい＝お前ちょっと落ち着け？

  失敗回数ではなく「ズレの大きさメーター」。 これを減らすのが学習の目的。

  地図で表現すると地形の高さ

- Backward

  各座標に「どっちに動け」と指示を出す、誤差を後ろから流す

  出力層 → linear → embeddingに向かって（いつもと逆方向です）

  「どっちに動かすか」を伝播する。

  地図で表現すると下り坂の方向検出らしい

- 勾配（Gradient）

  間違いが減る方向の矢印

  学習の時、誤差を最小化し、重みを最適化するための指標

  ライブハウスのPAさんがイコライザのつまみをどれくらい上げ下げするかのメモみたいなもん。もわっと上げたりシャキーンと上げたり温度があることもある。

  「この重みを 0.003 減らせ」みたいな微調整の指示。1回で劇的に変わらん。0.001ずつ削る地味作業。

- 学習率（Learning Rate）

  1回の更新でどれだけ空間を動かすかっていう割合、究極の話をするとこれ、1回でパーフェクトなモデルが出来上がると世界は激変する。現実？聞くな。

- Optimizer

  勾配（Gradient）が「こっち下れ！」って指示を出してきたときに、
  「で、どのくらい下る？どう下る？」を決めるやつ。

  勾配＝方向ナビ
  Optimizer＝運転手

  まっすぐ素直に行くか、ちょっと慣性つけるか、勢いを平均するか。
  つまり“下り方の戦略家”。

- Adam

  実は最初からいるけどよくわかってない、パッと見は聖書の(eva)の旦那

  今後なんか正体がわかったら追記する

## 空間関連

- ベクトル

  よく出てくるワードナンバーワン、1次元配列みたいな数の並びであったり方向の表現の一つだったり、百面相かお前は。

- 次元（Dimension）

  1次元は線、2次元は面、3次元は立体、それ以上は人間には観測できないけどコンピューターならN次元まで表現可能、だがそれとこれとは別の話。

- hidden\_dim

  Pythonで書くときに次元数を指定するときの変数名はだいたいこれ。

- ノルム（Norm）

  1つのベクトルの長さを何故かカッコつけて「ノルム( ･ิω･ิ)」とか言うのが頭のいい人たちのアイデンティティ、ノルムなんて飾りですよ、偉い人にはわからんのです。つまりジオングの足。

- ユークリッド距離

  直線距離、点Aと点Bがあってその間の直線距離のこと、ルート（√）とか使うやつね、直線距離をかっこよく言いたいならこれ使っていいいよ。

- 内積（Dot Product）

  2つのベクトルがどれだけ「同じ方向を向いているか」を測るやつ。

  ただの距離ではない。
  ただの角度でもない。

  イメージとしては：
  懐中電灯を1本持っていて、もう1本の矢印にどれだけ光が当たるかを見る感じ。

  ・向きが似ている → 光がたくさん当たる → 内積大
  ・横向き → 光があまり当たらない → 内積小
  ・逆向き → 光が裏側に当たる → 内積マイナス

  しかもベクトルが長いと光も強くなる。
  つまり「向き」と「勢い」の合わせ技。

- 角度

  学校で習う分度器のアレ

- 射影

  ベクトルを別のベクトルの方向に“影として落とした長さ”。

  懐中電灯を当てたときにできる影の「その方向成分だけ」を取り出す感じ。

  つまり：
  「どれだけその方向に含まれてるか？」を測るやつ。

  内積はこの“影の長さ”を実は計算してる。

- 回転不変性

  　・玉転がしても玉は玉。

  　・ｲｯﾇの画像を横にしてもｲｯﾇはｲｯﾇ

  　・空間を回転させても本質的な性質や値は変わらない

  そういうことを回転不変性と呼ぶ

  アニメとかでよくある記憶を失った人とか、人格変わっちゃった人を昔、親しかった人が「それでも君は変わってない！」とか言って昔の姿に戻る、アレのこと。

- 空間再構築

  学習の際に駅と線路が離れたり近づいたりする、崩壊が起こった箇所を再学習で新しい形に構築していくこと

- 空間崩壊

  学習の際に駅と線路が離れたり近づいたりする、いい感じになってた箇所が学習で変になっちゃうこと

- ノルム爆発

  宇宙が誕生した際に発生した爆発。

  のことではなく

  Linear固定でEmbeddingだけ動かせる状態を作って内積を「長さ」で稼ぐようになった末にEmbeddingがどんどん大きくなると：

  　・数値が不安定になる

  　・勾配が暴れやすくなる

  　・一部トークンだけ巨大化する

  これがビッグバン（違）

## 学習概念

- 訓練分布

  学習の際に用意されたデータ自体の傾向とか統計上こういう感じになってますよみたいな分布。

- 分布外（Out-of-Distribution）

  学習データの世界地図に載ってない場所。

  J地区にもE地区にもない単語やパターンが来た状態。

  モデルは「近い場所」に無理やり置こうとする。
  だから変な確信や誤答が生まれる。

- 破壊的忘却

  ファインチューニングとかで重み付け変えようとしたら確率変わって覚えてたこと忘れちゃった☆って状態。これが起きたモデルは直すの大変だろうね。

- 正則化

  空間が暴走しないように重りをつける仕組み、要は車がぶつからないようにオーライオーライする人、ずっと正規化（せいきか）と勘違いしてAIと話してた。こっちはせいそくか。

- Weight Decay

  重み（ベクトルの長さ）がデカくなりすぎないようにする減衰装置。

  ノルム爆発の予防策。

  「伸びすぎたらちょっと縮めますよ〜」っていう重り。

  ノルム固定とは違う。
  固定じゃなくて“伸びすぎた分を少し戻す”感じ。

- LayerNorm

  みんなのテンションを一旦そろえる装置。

  ベクトルの平均とばらつきを調整して、
  「今ちょっと暴れてるから落ち着け」とする。

  爆発を防ぎ、学習を安定させる。

  空間で言うと：
  一回全員を同じくらいの音量に整えるミキサー。

- 過学習

  同じ傾向になるデータをぶち込みすぎるとLLMはぶち込んだ同じ傾向データにどんどん寄ってく、そしたら返してほしい返答以上に過剰に偏った方針の返答が返ってきたり、逆にほしい返答から遠ざかったりすること

- 汎化

  あんまり尖った学習をしてしまうと偏った回答をし始めたりする、汎化は地形をなめらか〜にすることでLLM全体を安定させる方針みたいなもん。

  汎化-特化ってあったよね？抽象-具象の関係で。あれをLLM業界だとあんまり偏りすぎてない状態に学習傾向を均すこと。

## 出力制御

- 確率分布

  LLMが「This is a pen」を出力する際に

  This is aまでは出したから〜→次は…

  　penやな=0.7

  　dogやで=0.1

  　ThisThisThisThis...=0.001

  こんな感じで、推測する単語郡の確率の割合。

- 温度（Temperature）

  学習の際に先生が優しく教えるか（温度高い）、厳しく教えるか（温度低い）

  推論時に確率分布を尖らせるか緩めるか

  優しく教えるとモワッと変わる、厳しく教えるとトキンッと変わる

  MIXING作業とかでHot-Coldのつまみがある拡張機能とかあるよね？、あれ。

  主に推論時の調整ノブ

- トークン

  Emmbeddingに無数に存在してる1個の要素のことをトークンって呼んでる。

  でもー

  トークンはEmbeddingの中の要素じゃない。

  ・トークン = 分割された単位（単語やサブワード）

  ・Embedding = トークンを数値ベクトルに変換したもの

  文字列 → トークン化 → Embeddingベクトル化。トークンは駅名、Embeddingは駅の座標

- 推論（Inference）

  モデルがユーザーの入力に対してデータを出力するフェーズ

  embedding\
  → linear\
  → logits\
  → softmax\
  → 確率分布\
  → 出力

  だいたいこんなイメージ。上の概念フローは間違っては居るけど致命的な間違いではない。

- 学習フェーズ

  モデルにデータをぶち込むフェーズ、たくさんのデータを何回もぶち込むとLLMの内部データの重みが変わる。要はエサを食わせるフェーズ。

  学習でのみ使う

  → loss\
  → backward\
  → optimizer更新

- 確信度

  LLMが、これで返答が合ってる！と思って出力する割合。

  これが低いとモデル内部で答えが確定してない揺れている状態、曖昧な感じで悩んだような返答が来たりする、逆に高いと平然と合ってるって断定したり、嘘ついたりする。

- 内部確信

  LLMが推論の際にこういうことを返答しようという確信の度合い。

  （架空の人物）の誕生日は？→1月1日です

  これは日付を答えろという質問に「日付を答えればいいんだ」という確信から来る誤答。正解は「知らないっすｗｗｗだれっすかｗｗｗ」と答えればいいが「日付を回答すること」に引き摺られて発生する。

  ロジット差だけではなく、空間の濃さや構造も影響する、構造がロジットを生み、ロジットが確率を生む

- 事実確信

  上記、内部確信の例で調べた結果本当に架空の人物の誕生日が出てきた場合LLM内部で事実であることが確定すること、ただ、事実確信が低い、かつ、構造確信が高いとAIは嘘つくということがままある、初期はこれが多かった。

- メタ認知

  LLMが自分が何であるかを認識している事実。

  哲学的に書いてみたが要は「内部計算をどうやって算出して出力したかLLM自身がどれくらい知ってるか」って話。

- Hallucination

  幻覚、AIがバカにされるとき大抵この言葉が使われる、本当にバカなのは何でもかんでもハルシネーションのせいにするやつ。

## Attention関連

- Transformer

  単語同士の関係を同時に見る仕組み文章全体を同時に参照して重み計算する。

  行列計算を何回か実施して文章全体でどうなってんのかな〜結局何が言いたいんかな〜？って考えるための仕組み。

  駅線路例で例えると駅同士が無線で相談する装置、と書いてるが文章全体が何を伝えたいのかを数学的に読み取る方法とも言える

  これがないと…

  　h → 次は何や？

  だがこれがあると

  　h：ワイの次なんや？

  　e：前に誰おった？

  　l：文全体どうや？

  こうなる。

  ChatGPT登場以前のAIはこれがないので出力がいまいち噛み合っていなかった。

- Self-Attention

  飛行機が飛び立つときにCAさんが言うやつ、

  うそ、文章全体の主観を決める仕組み、「彼は怒っていない」って言葉があったとき「いない」にどうやって注目させるのかを実現してる仕組み

  「あってる！？あってる！？いみあってそ！？だいじょぶそ！？だいじょぶそ！？ぜんたいあってる！？せーーーーの！・・・はいおーけー・・・」ってログ漁ったらこう表現してた、つくづく私はアホなようだ。

  でも私は飛行機は修学旅行でしか乗ったことない。

- Query

  「今知りたいこと」って表現しろってAIに言われました

- Key

  「参照候補」って表現しろってAIに言われました

  Map\<Key,Value>のKey、わかんなかったらJavaでもやれ

- Value

  「情報本体」って表現しろってAIに言われました

  Map\<Key,Value>のValue、わかんなかったらJavaでもやれ

- Attention

  1.単語同士の関連スコアを出す（dot product）

  2.そのスコアを softmax で正規化

  3.重み付き平均を取る

  つまり：

  > どの単語をどれくらい見るか決める

  そういう仕組み。

- Attentionスコア

  指定文章のどこに主題を置くか、どこに注目して読むかの指標、単語同士の関連スコア。「彼は怒っていない」は"いない"を注目するためにどう計算するかのLinearから湧いてくるTransformerの源泉。

  AttentionのSoftmaxに関連する。

- AttentionのSoftmax

  例えばスコアが：

  　A: 10

  　B: 9

  　C: 8

  softmaxあり：

  　A: 0.66

  　B: 0.24

  　C: 0.09

  → 合計1\
  → 競争\
  → 主体が明確

  softmaxなし：

  　A: 10

  　B: 9

  　C: 8

  そのまま重みとして使うと？

  →スケールに依存

  →合計が不定

  →爆発しやすい

  →文脈がぼやける

  文脈の焦点を作る装置を実現できる（すごい！）

  これがないと：

  ・全部を見る

  ・主語が曖昧

  ・文脈が混ざる

## 実験操作

- Embedding凍結

  駅の位置を動かさないように固定すること。駅はふつう動かないがAIのEmbedding(単語)は普通にギャンギャン動くので動かないようにする

  こうすると学習時に語彙が固定される→実験しやすい！(視野が狭い言い方)

- Linear凍結

  線路の位置を動かさないように固定すること。さっきも言ったが線路も当然動かない、Linearが普通にギャンギャン動くので動かないようにする

  こうすると学習時に表現が固定される→実験しやすい！(視野が狭い言い方)

- ノルム固定

  ノルム爆発が起らないようにEmbeddingの長さを固定する、そうするとベクトルの大きさは一定（固定したからね）、方向だけ動かせるようになる。

  結果、駅の円周上を動くようになる。

  それって方向しか一致しないけどいいの？→いいんです！表現力が犠牲になれば（よくない）

- 非線形層

  空間を「回す・伸ばす・縮める」だけの人。

  どれだけ行列積を重ねても、Linear → Linear → Linearは結局 1回のLinearと同じ になる。

  つまり、いくら層を増やしても「直線的な世界」から出られない。

  ここに非線形（ReLUとか）が入ると空間が「折れる」紙を折るみたいに直線しか描けない世界から、角ができる世界に変わる。

  結果、表現力が爆増する。

- ReLU

  負の値は0にして正の値はそのままにするというマイナスが大嫌いなやつ

  ある軸を境に空間をパキッと切っちゃってマイナスとプラスで数値的に微妙な差を明確に遠く見せる技術的なやつ、絶対に前世は金持ち。

* 次元拡張

  hidden\_dimを増やせ！

* 空間制御

  この実験の大きな目的の一つ、LLM内部を観測し、分布やベクトルを制御する。

* 空間観測

  この実験の最初の目的、そもそも中身はどうなってんの？を観測、いきなり小さなモデルを用意するのではなく”helo”の4文字の観測から始めた

* 空間歪み

  別に役に立つ歪みであれば悪ではない。学習時に

  正解方向に引っ張る、不要方向を押しのける、分離したいものを引き離す、まとめたいものを近づける、これ全て歪み。だめな歪みは必要なEmbedding単語関係を離す、Linearで必要な単語が出ないようになる線路関係に変わる、空間自体がぐちゃぐちゃになる。

* 空間固定

  Embedding、Linear、Transformer、なんか層になっている特定層を固定すると固定している層以外の役割が観測できるようになる。

* 空間破壊

  空間歪みの結果、空間自体がぐちゃぐちゃになる。

  多分こうなったらLLMからの応答は人間からしたら支離滅裂に見える回答をする。

* 軸追加

  例えば怒り⇔喜びの軸に冷静を直交で追加する。とか感情に限らず言葉は複数の軸が混ざっているのでどこかでベクトルの軸を追加する。

## 数学的概念

- cosθ

  向きの一致度

  ↑↑＝1＝同じ方向

  ↑→＝0＝真横（直角）

  ↑↓＝-1＝逆向き

  ↑↑↓↓←→←→BA＝コナミコマンド

- 射影長

  直角三角形の2つの線の間にできる影の長さ。らしいけどなんかいまいちよくわかってない。どれやねん。お前は。と思ってるｗｗAIは文字でしか教えてくれないので伝わってない！ｗ

- スカラー

  向きを持たない“ただの量”。

  温度、時間、長さ、点数みたいなもの。

  ベクトルが矢印なら、スカラーは「強さメーター」。

  LLMで言うと：
  ロジット値とか確率とか“ただの数値”はスカラー。



- 行列

  普通にExcelのやつ

- 行列積

  行列の積（掛け算）は、左側の行列（m\*n）の列数と右側の行列（n\*l）の行数が一致する場合に定義され、左側の各行ベクトルと右側の各列ベクトルの内積を計算して新しい行列（m\*l）を作る演算。単なる成分ごとの積ではなく、線形変換の連続的な適用（合成）を意味し、順序（AB≠BA）が重要となる。とか言ってるけど全く意味わからんのでワイ語でいうと表同士をいい感じに掛け算すると新たな表が出てくるぜ。これは2つの表の最終的な変化を表してんねん。と理解。

  本質としては「ベクトルを別の空間に写像する装置」

  行列は「変換ルール表」。

  掛け算は「変換を適用」。

  合成は「変換を重ねる」。

- 転置

  行列の縦横を切り替える新入社員研修の時Javaで実装させられるForと配列で作る例のアレ。

- ロジット差

  ロッテ3-阪神3→どっちが勝つんや・・・

  ロッテ4-阪神3→まだまだ逆転できるで・・・

  ロッテ33-阪神4→なんでや阪神関係ないやろ

  つまり29点差が問題じゃなく「こりゃもう絶望や…」「明日やってもまた負けるやろw」ってのがロジット差

## そのほか出てきたキーワード

- 分布密度

  ぶっちゃけわからんが言葉のイメージだと「集まってる点の近さ。」

- 構造確信

  「AIは嘘をつく」って言われる一部、知りもしない人の誕生日を答えてと聞いたときに1/365で当てに行くってことをやってしまう際に学習された「日付を返せばいいんだ！」という強く学習されたテンプレ空間に沿って回答が寄ってしまうこと、知らなきゃ知らんでいいのにね。

- 競争構造

  結果が3つあったとする

  A：95点

  B：85点

  C：60点

  これをSoftmaxで競争させるとAがめっちゃ勝つ結果が出てくる、これがSoftmaxで演出される競争構造

- テンプレ空間

  学習した結果「こういうことをすればいいんだ」とLLM学んだ空間、ちょい抽象的すぎでは？ｗ

- クラスタ

  Embeddingでは単語駅が集まって暮らしてる

  J地区には日本語クラスタという集団を作って日本語たちが集まって線路を繋いでいる。

  E地区には英語クラスタという集団を作って英語たちが集まって線路を繋いでいる。

  そゆこと。（どゆこと？）

- 表現崩壊

  空間の整合性が壊れて「読めなくなる」状態。

  たとえば：

  　・Embedding（駅）が動きまくる

  　・でもLinear（線路）はそのまま

  するとどうなる？

  駅と線路が噛み合わない。
  電車がホームに来ない。
  方向はあるのに意味が拾えない。

  単語同士の関係がバラける
  本来近いはずの意味が離れる
  必要な単語が出てこない

  つまり：
  空間が“壊れる”というより
  「空間の整合性が取れなくなる」状態。

  人間目線では
  → 支離滅裂
  → 文脈が飛ぶ
  → なんか変

  でも内部的には
  方向が合ってないだけ。

  これが表現崩壊。

- 単調化

  表現の幅が消えて同じ単語しか使わなくなって固定化された癖を持った状態。語彙力を失った人。

- ノイズ

  学習時に入ってくる要らんデータ。

- 外れ値検出

  悪魔の証明をLLMがするときに想定された出力候補の中から「明らかに答えがもうないわ」って状態を見つけ出す手法、

  もっと短く言うと不確実性推定の際に「知らん」って判断するやり方の一つ。

- 不確実性推定

  知らないことを知らないと言えるのか言えないかの状態

  LLMは何でも知っているわけではないが一番近いものを頑張って出力するようにできている、そのため本来知らないものでも一番近いもので選び出そうとする

  実はLLMが苦手なこと「知らなければ知らないでいいのだ。」



---

# 🧪 TODO（次フェーズ実験メモ）

- linear固定でEmbeddingを変更「線路の向きに駅を寄せにいく」学習を実践していく今度はEmbeddingの移動距離を観測するゲームを行う、その前にしっかりAIと会話してどうなるのかの予測と知識をつけてから挑むこと。

※ 原則：1回につき1要素だけ変える（小さく観測する）

